{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhrn5O-qUYZ"
   },
   "source": [
    "# Import and misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbUpoArCqUYa",
    "outputId": "221aa57d-9e76-41a1-ca7b-2da58430d42d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_lite.utilities.seed:Global seed set to 228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, Union, List, Callable, Optional\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import pathlib\n",
    "import dataclasses\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchaudio\n",
    "from IPython import display as display_\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(228)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "812GwLfqqUYf"
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1DuQIyRqUYf"
   },
   "source": [
    "In this notebook we will implement a model for finding a keyword in a stream.\n",
    "\n",
    "We will implement the version with CRNN because it is easy and improves the model. \n",
    "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8PdhApeEh9pH"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TaskConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8)\n",
    "    hidden_size: int = 64\n",
    "    gru_num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA1gPmE1h9pI"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y2N8zcx9MF1X"
   },
   "outputs": [],
   "source": [
    "# !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "# !mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "12wBTK0mNUsG"
   },
   "outputs": [],
   "source": [
    "class SpeechCommandDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        if csv is None:\n",
    "            path2dir = pathlib.Path(path2dir)\n",
    "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
    "            \n",
    "            all_keywords = [\n",
    "                p.stem for p in path2dir.glob('*')\n",
    "                if p.is_dir() and not p.stem.startswith('_')\n",
    "            ]\n",
    "\n",
    "            triplets = []\n",
    "            for keyword in all_keywords:\n",
    "                paths = (path2dir / keyword).rglob('*.wav')\n",
    "                if keyword in keywords:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
    "                else:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
    "            \n",
    "            self.csv = pd.DataFrame(\n",
    "                triplets,\n",
    "                columns=['path', 'keyword', 'label']\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.csv = csv\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        instance = self.csv.iloc[index]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-1rVkT81Pk90"
   },
   "outputs": [],
   "source": [
    "dataset = SpeechCommandDataset(\n",
    "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "DFwhAXdfQLIA",
    "outputId": "0580c5a5-d1a8-4a5f-c61f-a7989ac2b27a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9b5e8e30-c4da-4ca5-a31d-51506d63ddc4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43061</th>\n",
       "      <td>speech_commands/bird/ccea893d_nohash_0.wav</td>\n",
       "      <td>bird</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>speech_commands/four/b5552931_nohash_0.wav</td>\n",
       "      <td>four</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31537</th>\n",
       "      <td>speech_commands/go/824e8ce5_nohash_0.wav</td>\n",
       "      <td>go</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10654</th>\n",
       "      <td>speech_commands/eight/39a12648_nohash_1.wav</td>\n",
       "      <td>eight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54871</th>\n",
       "      <td>speech_commands/no/41777abb_nohash_0.wav</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b5e8e30-c4da-4ca5-a31d-51506d63ddc4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9b5e8e30-c4da-4ca5-a31d-51506d63ddc4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9b5e8e30-c4da-4ca5-a31d-51506d63ddc4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              path keyword  label\n",
       "43061   speech_commands/bird/ccea893d_nohash_0.wav    bird      0\n",
       "17022   speech_commands/four/b5552931_nohash_0.wav    four      0\n",
       "31537     speech_commands/go/824e8ce5_nohash_0.wav      go      0\n",
       "10654  speech_commands/eight/39a12648_nohash_1.wav   eight      0\n",
       "54871     speech_commands/no/41777abb_nohash_0.wav      no      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUxfDJw1qUYi"
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dkmkxPWQqUYe"
   },
   "outputs": [],
   "source": [
    "class AugsCreation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.background_noises = [\n",
    "            'speech_commands/_background_noise_/white_noise.wav',\n",
    "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "            'speech_commands/_background_noise_/pink_noise.wav',\n",
    "            'speech_commands/_background_noise_/running_tap.wav'\n",
    "        ]\n",
    "\n",
    "        self.noises = [\n",
    "            torchaudio.load(p)[0].squeeze()\n",
    "            for p in self.background_noises\n",
    "        ]\n",
    "\n",
    "    def add_rand_noise(self, audio):\n",
    "\n",
    "        # randomly choose noise\n",
    "        noise_num = torch.randint(low=0, high=len(\n",
    "            self.background_noises), size=(1,)).item()\n",
    "        noise = self.noises[noise_num]\n",
    "\n",
    "        noise_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "        noise_energy = torch.norm(noise)\n",
    "        audio_energy = torch.norm(audio)\n",
    "        alpha = (audio_energy / noise_energy) * \\\n",
    "            torch.pow(10, -noise_level / 20)\n",
    "\n",
    "        start = torch.randint(\n",
    "            low=0,\n",
    "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
    "            size=(1,)\n",
    "        ).item()\n",
    "        noise_sample = noise[start: start + audio.size(0)]\n",
    "\n",
    "        audio_new = audio + alpha * noise_sample\n",
    "        audio_new.clamp_(-1, 1)\n",
    "        return audio_new\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
    "        augs = [\n",
    "            lambda x: x,\n",
    "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "            lambda x: self.add_rand_noise(x)\n",
    "        ]\n",
    "\n",
    "        return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClWThxyYh9pM",
    "outputId": "c5d11dd7-08c8-443e-de7d-909fb04a6246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_lite.utilities.seed:Global seed set to 228\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(228)\n",
    "\n",
    "indexes = torch.randperm(len(dataset))\n",
    "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
    "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
    "\n",
    "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
    "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PDPLht5fqUYe"
   },
   "outputs": [],
   "source": [
    "# Sample is a dict of utt, word and label\n",
    "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
    "val_set = SpeechCommandDataset(csv=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mmrJd8WIhkLP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vbPDqd6qUYj"
   },
   "source": [
    "### Sampler for oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rfnjRKo2qUYj"
   },
   "outputs": [],
   "source": [
    "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
    "\n",
    "def get_sampler(target):\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.float()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UM8gLmHeqUYj"
   },
   "outputs": [],
   "source": [
    "train_sampler = get_sampler(train_set.csv['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lyBqbxp0h9pO"
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        wavs = []\n",
    "        labels = []    \n",
    "\n",
    "        for el in data:\n",
    "            wavs.append(el['wav'])\n",
    "            labels.append(el['label'])\n",
    "\n",
    "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
    "        wavs = pad_sequence(wavs, batch_first=True)    \n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return wavs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8G9xPRVqUYk"
   },
   "source": [
    "###  Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6wGBMcQiqUYk"
   },
   "outputs": [],
   "source": [
    "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
    "                          shuffle=False, collate_fn=Collator(),\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
    "                        shuffle=False, collate_fn=Collator(),\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTlsn6cpqUYk"
   },
   "source": [
    "### Creating MelSpecs on GPU for speeeed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pRXMt6it56fW"
   },
   "outputs": [],
   "source": [
    "class LogMelspec:\n",
    "\n",
    "    def __init__(self, is_train, config):\n",
    "        # with augmentations\n",
    "        if is_train:\n",
    "            self.melspec = nn.Sequential(\n",
    "                torchaudio.transforms.MelSpectrogram(\n",
    "                    sample_rate=config.sample_rate,\n",
    "                    n_fft=400,\n",
    "                    win_length=400,\n",
    "                    hop_length=160,\n",
    "                    n_mels=config.n_mels\n",
    "                ),\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    "            ).to(config.device)\n",
    "\n",
    "        # no augmentations\n",
    "        else:\n",
    "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=config.sample_rate,\n",
    "                n_fft=400,\n",
    "                win_length=400,\n",
    "                hop_length=160,\n",
    "                n_mels=config.n_mels\n",
    "            ).to(config.device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # already on device\n",
    "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Pqkz4_gn8BiF"
   },
   "outputs": [],
   "source": [
    "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
    "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoAxmihY8yxr"
   },
   "source": [
    "### Quality measurment functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "euwD1UyuqUYk"
   },
   "outputs": [],
   "source": [
    "# FA - true: 0, model: 1\n",
    "# FR - true: 1, model: 0\n",
    "\n",
    "def count_FA_FR(preds, labels):\n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YHBUrkT1qUYk"
   },
   "outputs": [],
   "source": [
    "def get_au_fa_fr(probs, labels):\n",
    "    sorted_probs, _ = torch.sort(probs)\n",
    "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "        \n",
    "    FAs, FRs = [], []\n",
    "    for prob in sorted_probs:\n",
    "        preds = (probs >= prob) * 1\n",
    "        FA, FR = count_FA_FR(preds, labels)        \n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "    # plt.plot(FAs, FRs)\n",
    "    # plt.show()\n",
    "\n",
    "    # ~ area under curve using trapezoidal rule\n",
    "    return -np.trapz(FRs, x=FAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcEP5cEZqUYl"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cP_pFIsy5p2",
    "outputId": "e5178a47-f85f-48d8-d411-82ccd59a45d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
       "    (1): Flatten(start_dim=1, end_dim=2)\n",
       "  )\n",
       "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (attention): Attention(\n",
       "    (energy): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        energy = self.energy(input)\n",
    "        alpha = torch.softmax(energy, dim=-2)\n",
    "        return (input * alpha).sum(dim=-2)\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "config = TaskConfig()\n",
    "model = CRNN(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DmmSFvWaqUYn"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loader, log_melspec, device):\n",
    "    model.train()\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # run model # with autocast():\n",
    "        logits = model(batch)\n",
    "        # we need probabilities so we use softmax & CE separately\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UIeRbn4tqUYo"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(model, loader, log_melspec, device):\n",
    "    model.eval()\n",
    "\n",
    "    val_losses, accs, FAs, FRs = [], [], [], []\n",
    "    all_probs, all_labels = [], []\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        output = model(batch)\n",
    "        # we need probabilities so we use softmax & CE separately\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        all_probs.append(probs[:, 1].cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        val_losses.append(loss.item())\n",
    "        accs.append(\n",
    "            torch.sum(argmax_probs == labels).item() /  # ???\n",
    "            torch.numel(argmax_probs)\n",
    "        )\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "\n",
    "    # area under FA/FR curve for whole loader\n",
    "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
    "    return au_fa_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PpyvKwp0k3IU"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSNW-nZCJ4Q0"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8sVpHNoocgA",
    "outputId": "61cee7e0-408d-4bca-8d6e-79389259c9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
      "    (1): Flatten(start_dim=1, end_dim=2)\n",
      "  )\n",
      "  (gru): GRU(144, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (attention): Attention(\n",
      "    (energy): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = TaskConfig(hidden_size=32)\n",
    "model = CRNN(config).to(config.device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zedXm9dmINAE",
    "outputId": "d095783b-12b8-430b-9f4f-97e553ba1aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25387"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt2kjqC-IobK",
    "outputId": "c040e42c-b2e8-449e-c8e2-627b6a8ff658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20317, 46260,  4665,  ..., 61251,  6623, 28722])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "32oooz4lqUYo",
    "outputId": "733d702d-bd46-4535-a8ac-a71f19154e1d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.719572407313169e-05\n",
      "END OF EPOCH 49\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(228)\n",
    "\n",
    "for n in range(50):\n",
    "\n",
    "    train_epoch(model, opt, train_loader,\n",
    "                melspec_train, config.device)\n",
    "\n",
    "    au_fa_fr = validation(model, val_loader,\n",
    "                          melspec_val, config.device)\n",
    "    history['val_metric'].append(au_fa_fr)\n",
    "\n",
    "    clear_output()\n",
    "    plt.plot(history['val_metric'])\n",
    "    plt.ylabel('Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(au_fa_fr)\n",
    "\n",
    "    print('END OF EPOCH', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBkTUHZcVugz",
    "outputId": "1869aa2a-c1fd-4724-bf9a-5b108cb3696c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'val_metric': [0.0006111962232584869,\n",
       "              0.00047793491822445763,\n",
       "              0.0003516616114079219,\n",
       "              0.00021869271620536778,\n",
       "              0.0002083986966279328,\n",
       "              0.00016719874812786843,\n",
       "              0.0001518621508443856,\n",
       "              0.00015237535993636208,\n",
       "              0.00012922724286919092,\n",
       "              0.00011341324177921834,\n",
       "              0.00010952836830390811,\n",
       "              8.458998696240044e-05,\n",
       "              0.00010003400010234344,\n",
       "              8.879114045951011e-05,\n",
       "              8.39216216333148e-05,\n",
       "              8.606397121493749e-05,\n",
       "              7.330535448649925e-05,\n",
       "              6.295165943325305e-05,\n",
       "              6.5547542631041e-05,\n",
       "              7.057221769434549e-05,\n",
       "              7.438548059868228e-05,\n",
       "              7.598478335042291e-05,\n",
       "              9.210909691461378e-05,\n",
       "              6.660976610048067e-05,\n",
       "              6.207442993882817e-05,\n",
       "              5.8774376126467845e-05,\n",
       "              5.745554811103995e-05,\n",
       "              5.6554448426290574e-05,\n",
       "              6.773166504573156e-05,\n",
       "              5.4937243031806586e-05,\n",
       "              5.329616744699811e-05,\n",
       "              4.32945577010381e-05,\n",
       "              5.880421386437346e-05,\n",
       "              5.8404388176438305e-05,\n",
       "              4.922629999667309e-05,\n",
       "              4.3909215101893636e-05,\n",
       "              5.57130242173524e-05,\n",
       "              4.80208553852865e-05,\n",
       "              4.526384840280827e-05,\n",
       "              5.0043854015286774e-05,\n",
       "              4.209508063723263e-05,\n",
       "              3.876518908696671e-05,\n",
       "              4.35690648897697e-05,\n",
       "              4.4147917005138506e-05,\n",
       "              4.4714834025345076e-05,\n",
       "              4.382566943575793e-05,\n",
       "              4.059125864678996e-05,\n",
       "              4.657074132307392e-05,\n",
       "              4.913678678295627e-05,\n",
       "              3.719572407313169e-05]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E0ZS95_sZMcL"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'base_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRZB9KXyVvfa"
   },
   "source": [
    "### Reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1guSqniIfz54",
    "outputId": "cc7bf688-0d3d-405f-860f-a6963ffe1938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20317, 46260,  4665,  ..., 61251,  6623, 28722])\n",
      "tensor([40169, 61937, 54708,  ..., 33714, 52841, 16231])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:05, 18.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.719572407313169e-05"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = CRNN(TaskConfig(hidden_size=32)).to(config.device)\n",
    "base_model.load_state_dict(torch.load('base_model.pt'))\n",
    "\n",
    "print(val_indexes)\n",
    "print(train_indexes)\n",
    "\n",
    "validation(base_model, val_loader, melspec_val, config.device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
